
what agent impriovements can we realistically expect from this architecture?
are we over-engineering? 
i don't like the "install agents per project" feature: too much work configuring each project
( 4. Create install script for client projects)
agents are relatively small md files...simpler solutions?

no manual override for rating nesscary (too much work)

@Content remembers brand voice across projects: every project 
has his own Brand voice. NOT 1 brand for all projects!!

explain: 8. Agent memory

(MCP access still gated by project-level permissions): good!

this i really do not like: 
Multi-Project Safety (30+ projects in ~/DEV)
Only explicitly opted-in projects get agents. The install-hooks.sh script is the single gate:
too much config!!


https://github.com/dizid/agent-evaluation.git


This project is an AI staffing agency. Agents get deployed to client projects, 
auto-evaluated on every task, and continuously improved. The end goal: optimal agents with 
the best personas, tools, skills, and MCP configs.

Key discovery: Claude Code has a SubagentStop hook that fires every time a subagent completes. 
This is the reliable auto-eval mechanism â€” not CLAUDE.md instructions, not manual /rate.